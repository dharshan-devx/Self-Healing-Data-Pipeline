# ğŸš€ Self-Healing Data Pipeline ğŸš€

A production-grade approach to building **resilient, observable, and automated data pipelines** that detect, diagnose, and recover from failures without human intervention.

---

## âœ¨ Executive Summary

Modern businesses depend on continuous, trustworthy data to power analytics, personalization, and AI. Traditional pipelines often fail silently, recover slowly, and demand significant manual effortâ€”resulting in lost revenue, stale dashboards, and compliance risks.

**This project delivers a revolutionary *Self-Healing Data Pipeline*:** an orchestrated, observable, and governed flow that *automatically detects anomalies and failures*, *applies the right remediation strategy* (retry, fallback, quarantine, backfill), and *proves end-to-end data lineage*. It dramatically reduces Mean Time To Recovery (MTTR), protects critical SLAs/SLOs, and builds unwavering stakeholder trust in data.

---

## ğŸ’” The Problem (Why this matters)

### Business Symptoms
*   â° Late or missing dashboards; inaccurate KPIs during critical windows (e.g., finance close, campaign launch).
*   ğŸ’¥ Fragile batch jobs that break on upstream schema changes or API rate limits.
*   ğŸ¦¸ Incident resolution that relies on tribal knowledge, Slack threads, and one or two â€œpipeline heroes.â€
*   â“ Limited answers to simple questions like *â€œWhere did this number came from?â€* or *â€œWhat was the blast radius?â€*

### Root Causes
*   **Operational Fragility:** ğŸ•¸ï¸ One failing task often blocks all downstream tasks; manual reruns are error-prone.
*   **Low Observability:** ğŸ•µï¸ Logs scattered across systems; no single view of task health and data dependencies.
*   **Poor Lineage & Governance:** ğŸ“‰ Unclear sourceâ†’transformâ†’destination mapping; limited auditability.
*   **Change Volatility:** ğŸŒªï¸ Upstream schema drift, API instability, late files, and transient infra/network failures are constant threats.

### Impact
*   **Revenue & Cost:** ğŸ’¸ Missed personalization windows, stockouts, campaign misallocation; over-spend on on-call time.
*   **Trust:** ğŸ’” Business users lose confidence in data; teams resort to building shadow pipelines.
*   **Compliance:** ğŸ“ Weak traceability for audits and incident post-mortems.

---

## âœ… The Solution (What we built)

A **self-healing** data platform that seamlessly combines orchestration, lineage, and automated remediation.

### Core Capabilities

1.  **Proactive Detection:** ğŸ©º Health checks, data freshness SLAs, volume/profile monitors, and schema guards.
2.  **Smart Remediation:** ğŸ©¹ Tuned retries with jitter, circuit breakers, dynamic backoff, fallback sources, and quarantine/rewind for bad data.
3.  **Lineage & Blast-Radius Analysis:** ğŸ—ºï¸ End-to-end lineage (job, dataset, column) to quantify impact and drive safe rollbacks/backfills.
4.  **Idempotent, Re-runnable Tasks:** âœ… Safe to retry without duplicates; checkpointing and exactly-once/at-least-once strategies.
5.  **Governance-Ready:** ğŸ”’ Audit trails, rich metadata, and policy hooks for PII and regulatory requirements.

### Reference Architecture (deployed in this repo)

*   **Airflow:** ğŸŒ¬ï¸ Orchestration and scheduling of DAGs (pipelines), retries, SLAs, and task dependencies.
*   **PostgreSQL:** ğŸ˜ Durable metadata store (Airflow Metadatabase) and an example analytical sink.
*   **Marquez (OpenLineage):** ğŸ”— Operational metadata + data lineage for jobs and datasets.
*   **Kafka + Zookeeper (optional):** âš¡ Event streaming and dead-letter queues for real-time ingestion.
*   **Neo4j (optional):** ğŸŒ Graph store for advanced lineage queries or anomaly/impact exploration.

> Note: Kafka/Neo4j are optional depending on your specific use case. The self-healing patterns apply robustly to both batch and streaming scenarios.

ğŸ›  How Self-Healing Works (Step-by-step)
Detect: ğŸš¨
Liveness: Task heartbeat, scheduler health.
Freshness: Dataset X must update by 07:00 IST; alerts if late.
Volume/Null/Uniqueness: Profile expectations (e.g., row count Â±20%, null rate < 1%).
Schema: Strict/compatible schema checks; optional contract enforcement.
Classify (Root-Cause Hints): ğŸ§
Transient network/API issues vs. deterministic code/logic errors.
Upstream unavailability vs. downstream write/permission failures.
Data quality breach vs. structural (schema) drift.
Remediate: ğŸ©¹
Retry with backoff + jitter for transient issues.
Circuit break dependent tasks to avoid cascading failures.
Fallback Source (e.g., cached snapshot, secondary API region) when the primary is down.
Quarantine suspect batches to a Dead-Letter area; continue healthy partitions.
Backfill once upstream stabilizes; compute only missing partitions (idempotent).
Prove & Learn: ğŸ“š
Emit OpenLineage events to Marquez for unparalleled traceability.
Capture incident metadata and outcomes; continuously update runbooks and policies.
ğŸ’¡ Example Failure Scenarios & Responses
A) API Rate-Limited During Peak Hours â±ï¸
Detection: 429 responses spike + task SLA at risk.
Action: Backoff and retry with jitter; switch to cached snapshot for read-only features; resume primary once the window clears.
Outcome: No missed ingestion; downstream freshness maintained within SLA.
B) Late File Arrival from Vendor ğŸ“¤
Detection: Freshness breach on s3://vendor/d=date by 06:30 IST.
Action: Pause downstream joins; ingest previous day as placeholder; auto-backfill once the file lands; mark partitions reconciled.
Outcome: Dashboards stay mostly current; final numbers reconcile automatically.
C) Schema Drift (new column added) ğŸ“Š
Detection: Contract mismatch on the extract step.
Action: Route batch to quarantine; continue ingesting other sources; notify owner; apply transform shim (ignore/new column mapping); reprocess.
Outcome: No pipeline-wide outage; controlled adaptation.
D) Downstream Warehouse Write Failure ğŸ’¾
Detection: Insert timeout / permission error.
Action: Retry with exponential backoff; if persistent, write to staging; open ticket with lineage context; enable read path from last good partition.
Outcome: Minimal downtime; clear blast-radius view for responders.
âš™ï¸ Operational Model
SLAs & SLOs
Freshness SLOs per dataset (e.g., D+1 by 07:00 IST; streaming p95 end-to-end < 2 minutes).
Reliability SLOs per DAG (e.g., >99.5% successful daily runs).
Error Budget to govern changes and deployments.
Idempotency & Exactly-Once
Deterministic keys and upserts; watermarking/checkpointing for replay.
Partitioned processing (by date/hour) to enable safe partial reruns.
Observability
Airflow task states, SLAs, retries, and logs.
Marquez/OpenLineage for datasetâ†”jobâ†”column lineage and impact analysis.
Metrics: success rate, retry rate, time-to-detect, MTTR, number of quarantined batches.
ğŸ” Governance, Security & Compliance
Access Control: ğŸ‘® Least-privilege for orchestrator, storage, and sinks.
Data Classification: ğŸ·ï¸ Tag PII/regulated datasets; restrict propagation.
Auditability: ğŸ“ Lineage + run history + config versioning.
Privacy: ğŸ›¡ï¸ Encrypt in transit (TLS) and at rest; key management via vault/KMS.
ğŸ“ˆ What This Delivers to the Business
Faster Recovery: ğŸš€ MTTR dramatically reduced via automation and targeted remediation.
Protected SLAs: â° Predictable data freshness for executive dashboards and critical ML features.
Traceability: ğŸ” Know precisely which reports and models were impacted, down to the column level.
Lower Ops Cost: ğŸ’° Fewer midnight pages; fewer expensive full-pipeline reruns.
Trust: â­ Stakeholders regain unwavering confidence in data outputs.
Sample ROI Levers
Avoid 2â€“4 hours/day of manual incident handling, freeing up engineers.
Prevent missed campaign windows or erroneous decisions due to stale data.
Faster onboarding of new pipelines due to consistent patterns and governance.
ğŸŒ Scope & Extensibility
In Scope: Batch orchestration, optional streaming, lineage, health checks, automated remediation, quarantine/backfill patterns.
Out of Scope (baseline): Real-time schema registry, advanced ML-driven anomaly detection (can be easily integrated as an extension), lakehouse optimization.
Easy Extensions
Integrate a Data Quality framework (e.g., Great Expectations) to formalize and automate data quality tests.
Add Slack/Teams on-call routing with incident tickets pre-filled from lineage context.
Promote Kafka dead-letter topics for automated replay mechanisms.
Use Neo4j to explore lineage/impact as a powerful graph at scale.
â“ FAQ (for non-technical stakeholders)
Q: Does this remove the need for data engineers?
A: Absolutely not! It removes repetitive firefighting and tedious manual tasks, allowing engineers to focus on building innovative features, optimizing performance, and strengthening data governance.
Q: Can it handle our existing pipelines?
A: Yes, the underlying self-healing patterns are technology-agnostic. Apache Airflow, used for orchestration, can manage tasks written in Python, SQL, Spark, and more. We standardize retries, idempotency, and lineage across diverse pipeline technologies.
Q: How do we know it worked during an incident?
A: Every single remediation actionâ€”from a retry to a quarantine or a backfillâ€”is meticulously logged and linked within the data lineage. Intuitive dashboards provide real-time visibility into data freshness, retry rates, quarantined batches, and backfill progress by dataset, ensuring full transparency.
Q: Is it safe for regulated data (e.g., PII, financial data)?
A: Emphatically yes! Governance, security, and compliance are first-class citizens in this design. This includes robust lineage and audit trails, comprehensive encryption for data in transit (TLS) and at rest, and strict role-based access controls to meet regulatory requirements.
ğŸ“š Glossary
DAG: Directed Acyclic Graph; defines the order and dependencies of tasks within a data pipeline.
Lineage: A complete, auditable trace of dataâ€™s origin, transformations, and destinations.
Quarantine/Dead-Letter: A secure holding area for suspect, corrupted, or failed records or batches, preventing them from contaminating downstream systems.
Idempotent: A task is idempotent if running it multiple times produces the exact same result as running it once, without any unintended side effects. Crucial for safe retries and reruns.
SLA/SLO: Service Level Agreement/Objectiveâ€”measurable commitments on aspects like data freshness, reliability, or latency.
âœ… Decision Checklist for Adopting Self-Healing
Do we have measurable data freshness targets for key datasets?
Are our existing tasks idempotent with clear partitioning and checkpoints?
Do we emit comprehensive lineage and health metrics for every job/dataset?
Are our remediation actions automated and auditable, not manual?
Is there a clear, owned runbook for exceptions that truly cannot be automated?
ğŸš€ Self-Healing Data Pipeline â€” Setup Guide
This section explains all commands required to set up, configure, and run the Self-Healing Data Pipeline from scratch.
It also includes common error fixes so you wonâ€™t get stuck!
ğŸ“Œ Prerequisites
Before starting, make sure you have installed:
Docker â†’ Download
Docker Compose â†’ Install (usually comes with Docker Desktop)
Git â†’ Download
Python 3.10+ (optional, only if you plan to run Airflow CLI commands locally outside of Docker)
ğŸ“‚ 1. Clone the Repository
First, clone the project repository to your local machine:
code
Bash
git clone https://github.com/<your-username>/self-healing-data-pipeline.git
cd self-healing-data-pipeline
ğŸ³ 2. Build and Start the Containers
Navigate into the cloned directory and bring up all the services using Docker Compose:
code
Bash
docker-compose up -d --build
âœ… What this command does:
docker-compose up: Creates and starts containers for all services defined in docker-compose.yml.
-d: Runs the containers in "detached" mode, meaning they run in the background.
--build: Forces Docker to rebuild images for all services, ensuring you have the latest code. This is crucial for initial setup.
ğŸ“Œ 3. Check Container Status
Verify that all your Docker containers are running as expected:
code
Bash
docker ps
You should see active services like:
airflow-webserver
airflow-scheduler
postgres
airflow-worker (if using Celery executor, which is common)
â³ 4. Check Airflow Webserver Health
The Airflow Webserver needs a moment to initialize. Check its health status:
code
Bash
docker inspect --format='{{.State.Health.Status}}' airflow-webserver
If it shows: âœ… healthy â†’ proceed to the next step.
If it shows: â³ starting â†’ please wait for a few more minutes (1-3 mins).
If it shows: âŒ unhealthy â†’ refer to Error Fix #1 below.
ğŸ‘¤ 5. Create an Admin User
You'll need an admin user to log into the Airflow UI. Run this command to create one:
code
Bash
docker-compose exec airflow-webserver airflow users create \
    --username admin \
    --firstname YourName \
    --lastname Admin \
    --role Admin \
    --email admin@example.com \
    --password admin
âœ… One-liner Version (for convenience):
code
Bash
docker-compose exec airflow-webserver airflow users create --username admin --firstname YourName --lastname Admin --role Admin --email admin@example.com --password admin
Note: You can replace YourName, Admin, admin@example.com, and admin with your desired values.
ğŸŒ 6. Access the Airflow UI
Once the webserver is healthy and the admin user is created, open your web browser and go to:
ğŸ”— http://localhost:8080
Use the credentials you just created:
Username: admin
Password: admin
You should now see the Airflow dashboard, ready to manage your self-healing pipelines!
ğŸ”„ 7. Restart Services (If Needed)
If you make any configuration changes (e.g., to docker-compose.yml or Airflow settings), it's good practice to restart the services:
code
Bash
docker-compose down && docker-compose up -d
ğŸ“œ 8. View Live Logs
For real-time debugging or monitoring, you can stream logs from your services:
For the Airflow Webserver:
code
Bash
docker-compose logs -f airflow-webserver
For all services (useful for overall health):
code
Bash
docker-compose logs -f
ğŸ§¹ 9. Stop & Remove All Containers
When you're done or want to clean up your environment:
Stop and remove containers (keeps volumes/data):
code
Bash
docker-compose down
Stop and remove containers and associated volumes (clears database, starts fresh):
code
Bash
docker-compose down -v
Use -v to ensure a completely clean start, removing any persistent data, which is often recommended for troubleshooting.
ğŸ›  Common Errors & Fixes
Don't worry, setup sometimes comes with a few bumps! Here are solutions to common issues:
âŒ Error 1: Airflow Webserver Stuck on starting
code
Bash
docker inspect --format='{{.State.Health.Status}}' airflow-webserver
# Output: starting (or unhealthy)
Fix: This often means a cached volume or an incomplete build.
code
Bash
docker-compose down -v # Remove containers AND all associated volumes
docker-compose up -d --build # Rebuild and start fresh
This is the most effective way to ensure a clean start by removing cached data volumes that might be causing issues.
âŒ Error 2: "No Module Named Airflow" (inside a container log)
This typically indicates an issue during the image build process where Airflow or its dependencies weren't installed correctly.
Fix: Force a rebuild of your Docker images without using cache.
code
Bash
docker-compose build --no-cache
docker-compose up -d
âŒ Error 3: Database Migration Issues (sqlalchemy.exc.OperationalError: could not connect to database)
This often happens if the PostgreSQL container isn't fully ready when Airflow tries to connect, or if the database is in a bad state.
Fix: A full cleanup and restart usually resolves this.
code
Bash
docker-compose down -v # Remove containers AND all associated volumes (including DB data)
docker-compose up -d # Restart everything, Airflow will re-initialize the DB
âŒ Error 4: Port 8080 Already in Use
If another application is already using port 8080 on your machine, Airflow's webserver won't be able to start.
Fix (Windows):
code
Bash
netstat -ano | findstr :8080  # Find the process ID (PID) using port 8080
taskkill /PID <PID> /F        # Kill that process (replace <PID>)
docker-compose up -d
Fix (macOS/Linux):
code
Bash
sudo lsof -i :8080            # Find the process ID (PID) using port 8080
kill -9 <PID>                 # Kill that process (replace <PID>)
docker-compose up -d
âš¡ Quick Start Summary (Cheat Sheet)
For the seasoned pros who just need the essentials:
code
Bash
# 1. Clone the repository
git clone https://github.com/<your-username>/self-healing-data-pipeline.git
cd self-healing-data-pipeline

# 2. Build and start all services
docker-compose up -d --build

# 3. Create an admin user for the Airflow UI
docker-compose exec airflow-webserver airflow users create \
    --username admin --firstname YourName --lastname Admin \
    --role Admin --email admin@example.com --password admin

# 4. Open Airflow UI in your browser
open http://localhost:8080 # or manually navigate
ğŸ¯ Final Notes
This project leverages Apache Airflow for robust orchestration and scheduling.
It is meticulously designed to self-heal by intelligently retrying failed tasks, quarantining bad data, and automatically recovering pipelines, minimizing human intervention.
All core configurations reside in docker-compose.yml and the dags/ directory. Explore these files to understand and customize your pipelines.
ğŸŒŸ Contributions
We welcome contributions from the community! If you have ideas for improvements, bug fixes, or new features, please feel free to:
Fork the repository.
Create a new branch (git checkout -b feature/your-feature-name).
Make your changes and commit them (git commit -m 'feat: Add amazing new feature').
Push to your fork (git push origin feature/your-feature-name).
Open a Pull Request with a clear description of your changes.
Please ensure your code adheres to existing style guidelines and includes appropriate tests.
ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.
ğŸ™ Acknowledgements
Inspired by the principles of robust data engineering and DevOps.
Built upon the incredible work of the Apache Airflow, PostgreSQL, and OpenLineage communities.
Created By Sondi Dharshan
Copyright Â© 2025. All rights reserved.
